# Multimodal Transformer for Affect Analysis in Human-Virtual Agent Dyadic Interactions

## Project Overview
This project aims to **automate affect analysis** by predicting **Pleasure, Arousal, and Dominance (PAD)** values from **non-intrusive multimodal data** (video and audio). Traditional affect recognition methods **ignore Dominance** or rely on **intrusive sensor-based approaches** (EEG, HRV). Our solution uses a **transformer-based late fusion model** to integrate information from video and audio without requiring wearable sensors.

### ğŸ”¹ Key Features
- **PAD Prediction**: Predicts **Pleasure, Arousal, and Dominance** from video and audio.
- **Multimodal Approach**: Uses both **video (VideoViT)** and **audio (Wav2Vec2)** models.
- **Late Fusion Transformer Model**: Dynamically assigns importance to audio/video based on context.
- **Transfer Learning**: Pretrained models on DEAP dataset are fine-tuned on MITHOS dataset.
- **Real-time Emotion Analysis**: Enables real-time affect prediction without manual intervention.

---

## ğŸ“‚ Project Directory Structure
To keep the repository well-organized, the following directory structure is used:

```
â”‚â”€â”€ src/                     # Main source code
â”‚   â”œâ”€â”€ audio/               # Audio processing pipeline
â”‚   â”‚   â”œâ”€â”€ audio_model.py   # Wav2Vec2-based model for audio PAD prediction
â”‚   â”‚   â”œâ”€â”€ audio_data_loader.py   # Loads the MITHOS dataset for audio
â”‚   â”‚   â”œâ”€â”€ audio_train.py   # Training script for audio model
â”‚   â”œâ”€â”€ video/               # Video processing pipeline
â”‚   â”‚   â”œâ”€â”€ video_model.py   # ViViT-based model for video PAD prediction
â”‚   â”‚   â”œâ”€â”€ video_data_loader.py   # Loads the DEAP/MITHOS dataset for video
â”‚   â”‚   â”œâ”€â”€ video_train.py   # Training script for video model
â”‚   â”œâ”€â”€ fusion/              # Fusion model pipeline
â”‚   â”‚   â”œâ”€â”€ fusion_model.py  # Transformer-based fusion model
â”‚   â”‚   â”œâ”€â”€ fusion_data_loader.py  # Loads preprocessed audio and video features
â”‚   â”‚   â”œâ”€â”€ fusion_train.py  # Training script for fusion model
â”‚â”€â”€ configs/                 # Configuration files (YAML)
â”‚â”€â”€ data/                    # Data storage (if applicable)
â”‚â”€â”€ results/                 # Logs, results, and output files
â”‚â”€â”€ requirements.txt         # List of dependencies
â”‚â”€â”€ README.md                # Project documentation
â”‚â”€â”€ LICENSE                  # License for open-source usage
```

---

## ğŸ”§ Installation & Setup

### ğŸ”¹ Prerequisites
- **Docker (Podman GPU)**: Used for running the model in a containerized environment.
- **Python 3.8+** with **pip** and **virtual environments**.

### ğŸ”¹ Steps to Set Up
1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/multimodal-affect-analysis.git
   cd multimodal-affect-analysis
   ```

2. **Set up a virtual environment** (optional but recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Start Docker Container** (Ensure GPU access if required):
   ```bash
   podman-gpu -it -v /home/rodricks/<someFolder>:/workspace:z -v /mnt/datasets/mithos/video_data:/video_data nvcr.io/nvidia/pytorch:22.05-py3
   ```

5. **Manually install OpenCV and dlib (inside Docker)**:
   ```bash
   pip install opencv-python==4.5.3.56 dlib
   ```

---

## â–¶ï¸ How to Run the Model

### ğŸ”¹ Train the Audio Model
```bash
python src/audio/audio_train.py
```

### ğŸ”¹ Train the Video Model (Pretrain on DEAP)
```bash
python src/video/video_train.py
```

### ğŸ”¹ Fine-tune Video Model on MITHOS
```bash
python src/video/train_mithos.py
```

### ğŸ”¹ Train the Fusion Model
```bash
python src/fusion/fusion_train.py
```

---

## ğŸ“Š Evaluation
After training, evaluate the models:

### ğŸ”¹ Evaluate the Audio Model
```bash
python src/audio/audio_evaluate.py
```

### ğŸ”¹ Evaluate the Video Model
```bash
python src/video/video_evaluate.py
```

### ğŸ”¹ Evaluate the Fusion Model
```bash
python src/fusion/fusion_evaluate.py
```

---

## ğŸ“ˆ Results & Performance
The performance of the models was evaluated using:
- **Mean Absolute Error (MAE)**
- **Range-2 Accuracy**
- **Pearsonâ€™s Correlation Coefficient (PCC)**
- **Mean Squared Error (MSE)**

### ğŸ”¹ Key Findings
- **Fusion Model performs better** than individual audio/video models.
- **Audio is more effective** in predicting **Pleasure and Arousal**.
- **Video is more effective** in predicting **Dominance**.

---

## ğŸ“ Citation
If you use this work, please cite:
```bibtex
@thesis{Rodricks2024,
  author = {Senorita Rodricks},
  title = {Multimodal Transformer for Affect Analysis in Human-Virtual Agent Dyadic Interactions},
  school = {Saarland University},
  year = {2024}
}
```

---

### ğŸš€ **Next Steps**
1. **Copy and paste this README.md into your GitHub repository**.
2. **Test all commands** to ensure they run without issues.
3. **Let me know** if you'd like any modifications!
